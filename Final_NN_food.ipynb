{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spaghetti!\n",
      "Python 3.7.11\n",
      "Num GPUs Available 0\n",
      "tf.Tensor(-948.1714, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('Spaghetti!')\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "!python --version\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available\", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 bad training samples\n",
      "Removed 0 training samples with text\n",
      "30612 training labels loaded\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# Load the training images\n",
    "#####################################################\n",
    "\n",
    "# Load the training images and class labels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "IMG_SIZE = 299  # VGG16 224, InceptionV3 299\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 15\n",
    "\n",
    "# Classes which should be removed from the training set\n",
    "REMOVE_LABELS = [] #['28', '55']\n",
    "\n",
    "# Filter for a subset of labels in the images to load\n",
    "FILTER_LABEL = []\n",
    "\n",
    "QUICK_DIRTY = False\n",
    "if QUICK_DIRTY:\n",
    "    IMG_SIZE = 112\n",
    "    BATCH_SIZE = 40\n",
    "    EPOCHS = 10\n",
    "    FILTER_LABEL = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "# Local data directories\n",
    "DATA_DIR = '/Users/dj/Documents/GitHub/Data/food-recognition-challenge-2021'\n",
    "TRAIN_DIR = DATA_DIR + '/train_set/train_set/'\n",
    "TEST_DIR = DATA_DIR + '/test_set/test_set/'\n",
    "MODEL_DIR = DATA_DIR + '/models/'\n",
    "\n",
    "# Maximum number of images to load (there are 30k), set to 0 for all\n",
    "MAX_IMAGE = 0\n",
    "\n",
    "# Load all the training labels\n",
    "train_labels = pd.read_csv(DATA_DIR + '/train_labels.csv', dtype={'label': object})\n",
    "if (len(FILTER_LABEL) > 0):\n",
    "    train_labels = train_labels[train_labels['label'].isin(FILTER_LABEL)].copy().reset_index()\n",
    "\n",
    "# Remove bad classes from the training data\n",
    "count = len(train_labels)\n",
    "train_labels = train_labels[~train_labels['label'].isin(REMOVE_LABELS)]\n",
    "print('Removed {} bad training samples'.format(count - len(train_labels)))\n",
    "NUM_CLASSES = len(train_labels.groupby('label').count())\n",
    "\n",
    "# Remove images with text from the training data\n",
    "# images_with_text = pd.read_csv('./images_with_text.csv')\n",
    "count = len(train_labels)\n",
    "#train_labels = train_labels[~train_labels['img_name'].isin(images_with_text['img_name'])]\n",
    "print('Removed {} training samples with text'.format(count - len(train_labels)))\n",
    "NUM_CLASSES = len(train_labels.groupby('label').count())\n",
    "\n",
    "# Shuffle the training labels\n",
    "train_labels = shuffle(train_labels, random_state=3422545)\n",
    "\n",
    "print('{} training labels loaded'.format(len(train_labels)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24489 validated image filenames belonging to 80 classes.\n",
      "Found 6123 validated image filenames belonging to 80 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# Create the data generators\n",
    "#####################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam # adam_v2\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "\n",
    "# Create a training and test set generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.0,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_split = int(len(train_labels)*0.8)\n",
    "train_dataframe = train_labels[:train_split][['img_name','label']]\n",
    "val_dataframe = train_labels[train_split:][['img_name','label']]\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_dataframe,\n",
    "    x_col = 'img_name',\n",
    "    y_col = 'label',\n",
    "    directory=TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val_dataframe,\n",
    "    x_col = 'img_name',\n",
    "    y_col = 'label',\n",
    "    directory=TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_datagen.flow_from_directory(\n",
    "        TEST_DIR,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "# Early stopping callback, stops the training if there's no more progress\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_acc', \n",
    "    patience=3, \n",
    "    min_delta=0.001, \n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Learning rate scheduler callback, decreases the LR during training\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch == 3:\n",
    "        lr = lr/10       # After 3 epochs, divide by 10\n",
    "    \n",
    "    if epoch == 6:\n",
    "        return lr/10\n",
    "\n",
    "    if epoch == 10:\n",
    "        return lr/10\n",
    "        \n",
    "    return lr\n",
    "\n",
    "lr_scheduler_callback = LearningRateScheduler(\n",
    "    lr_scheduler\n",
    ")\n",
    "\n",
    "# Model checkpoint callback, stores the model every few epochs during training\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    MODEL_DIR + 'model_checkpoint',\n",
    "    monitor=\"val_acc\",\n",
    "    verbose=1,\n",
    "    save_weights_only=False,\n",
    "    mode=\"auto\",\n",
    "    # save_freq=\"epoch\"\n",
    ")\n",
    "\n",
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"acc\"])\n",
    "    plt.plot(hist.history[\"val_acc\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Train Xception model\n",
    "##################################################\n",
    " \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define Tensorboard callback with dedicated log directory\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tb_callback = TensorBoard(log_dir=log_dir, update_freq=1)\n",
    "\n",
    "def InitializeXceptionModel():\n",
    "    # Defining the pretrained base model\n",
    "    base = Xception(include_top=False, weights='imagenet', input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "    x = base.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # On more dense layers + dropout, didn't improve at all\n",
    "    #x = Dense(1024, activation=\"relu\")(x) # Adding dense 512 + dropout lowered to 59%\n",
    "    #x = Dropout(0.1)(x)\n",
    "    # Defining the head of the model where the prediction is conducted\n",
    "    head = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    # Combining base and head \n",
    "    model = Model(inputs=base.input, outputs=head)\n",
    "\n",
    "    return model\n",
    "\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    if input('Train a fresh model?') == 'y':\n",
    "        print('Discarding current model')\n",
    "        model = InitializeXceptionModel()\n",
    "else:\n",
    "    model = InitializeXceptionModel()\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "hist = model.fit(\n",
    "    train_generator, \n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=int(len(train_dataframe)/BATCH_SIZE/2), \n",
    "    validation_data=val_generator, \n",
    "    validation_steps=int(len(val_dataframe)/BATCH_SIZE/2),\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, tb_callback, lr_scheduler_callback, model_checkpoint_callback])\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Train VGG16 model\n",
    "##################################################\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "\n",
    "# Define Tensorboard callback with dedicated log directory\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tb_callback = TensorBoard(log_dir=log_dir, update_freq=1)\n",
    "\n",
    "def InitializeVGG16Model():\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3), )\n",
    "    base_model.trainable = False ## Not trainable weights\n",
    "\n",
    "    flatten_layer = layers.Flatten()\n",
    "    dense_layer_1 = layers.Dense(1024, activation='relu')\n",
    "    dropout_layer_1 = Dropout(0.1)\n",
    "    dense_layer_2 = layers.Dense(512, activation='relu')\n",
    "    prediction_layer = layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        flatten_layer,\n",
    "        dense_layer_1,\n",
    "        dropout_layer_1,\n",
    "        dense_layer_2,\n",
    "        prediction_layer\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    if input('Train a fresh model?') == 'y':\n",
    "        print('Discarding current model')\n",
    "        model = InitializeVGG16Model()\n",
    "else:\n",
    "    model = InitializeVGG16Model()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['acc'],\n",
    ")\n",
    "\n",
    "hist = model.fit(\n",
    "    train_generator, \n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=int(len(train_dataframe)/BATCH_SIZE)/4, \n",
    "    validation_data=val_generator, \n",
    "    validation_steps=int(len(val_dataframe)/BATCH_SIZE/4),\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, tb_callback])\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Train InceptionV3 model\n",
    "##################################################\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import layers\n",
    "from PIL import Image\n",
    "import datetime\n",
    "\n",
    "# Define Tensorboard callback with dedicated log directory\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tb_callback = TensorBoard(log_dir=log_dir, update_freq=1)\n",
    "\n",
    "def InitializeInceptionV3Model():\n",
    "    pre_trained_model = InceptionV3(input_shape = (IMG_SIZE, IMG_SIZE, 3), include_top=False, weights = 'imagenet')\n",
    "\n",
    "    for layer in pre_trained_model.layers: \n",
    "        layer.trainable=False\n",
    "\n",
    "    x = layers.Flatten()(pre_trained_model.output)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = keras.Model(pre_trained_model.input, x)\n",
    "    return model\n",
    "\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    if input('Train a fresh model?') == 'y':\n",
    "        print('Discarding current model')\n",
    "        model = InitializeInceptionV3Model()\n",
    "else:\n",
    "    model = InitializeInceptionV3Model()\n",
    "    \n",
    "model.compile(\n",
    "    #optimizer = RMSprop(learning_rate=0.0001), \n",
    "    #optimizer = RMSprop(learning_rate=0.00005), \n",
    "    optimizer = RMSprop(learning_rate=0.000001), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['acc'])\n",
    "\n",
    "hist = model.fit(\n",
    "    train_generator, \n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=int(len(train_dataframe)/BATCH_SIZE/4), \n",
    "    validation_data=val_generator, \n",
    "    validation_steps=int(len(val_dataframe)/BATCH_SIZE/4),\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, tb_callback])\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Fine tune InceptionV3 model\n",
    "##################################################\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(learning_rate=0.000005, momentum=0.9),\n",
    "                            loss='categorical_crossentropy',\n",
    "                            metrics=['acc'])\n",
    "\n",
    "hist = model.fit(\n",
    "    train_generator, \n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=int(len(train_dataframe)/BATCH_SIZE/4), \n",
    "    validation_data=val_generator, \n",
    "    validation_steps=int(len(val_dataframe)/BATCH_SIZE/4),\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, tb_callback])                            \n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#  Save the model\n",
    "#######################################################\n",
    "\n",
    "# TODO: model name in filename\n",
    "model.save(MODEL_DIR + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \n",
    "    'model.' + 'xception' +\n",
    "    '.' + str(NUM_CLASSES) + 'c.' +\n",
    "    str(IMG_SIZE) + 'px.69p-removed-0-classes.remove-0-ocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#  Load a model\n",
    "#######################################################\n",
    "from tensorflow.keras import models\n",
    "\n",
    "#model = models.load_model(MODEL_DIR + 'model_checkpoint')\n",
    "model = models.load_model(MODEL_DIR + '20211205-122437model.xception.80c.299px.68p-2-classes-removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#  Classify the test set\n",
    "#######################################################\n",
    "\n",
    "# Classify\n",
    "test_generator.reset()\n",
    "probabilities = model.predict(test_generator)\n",
    "filenames = test_generator.filenames.copy()\n",
    "for i in range(0,len(filenames)):\n",
    "    filenames[i] = filenames[i][filenames[i].rfind('\\\\')+1:]\n",
    "predicted_class_indices = np.argmax(probabilities, axis=-1)\n",
    "\n",
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "results=pd.DataFrame({\"img_name\":filenames,\n",
    "                      \"label\":predictions})\n",
    "\n",
    "results.to_csv(DATA_DIR + '/predictions.csv', index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#  Look for confusing training images\n",
    "#######################################################\n",
    "\n",
    "train_set_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_set_generator = train_set_datagen.flow_from_dataframe(\n",
    "    train_labels,\n",
    "    x_col = 'img_name',\n",
    "    y_col = 'label',\n",
    "    directory=TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    batch_size=100,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Classify\n",
    "train_set_generator.reset()\n",
    "#probabilities = model.predict(train_set_generator, batch_size=32, verbose=1, steps=10, workers=2)\n",
    "probabilities = model.predict(train_set_generator, verbose=1, workers=2)\n",
    "In [ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#  Display the confusing training images\n",
    "#######################################################\n",
    "\n",
    "from PIL import Image\n",
    "fig, axs = plt.subplots(ncols=2, nrows=8)\n",
    "fig.set_size_inches(15, 30)\n",
    "\n",
    "# Show images that were hard to classify\n",
    "confidence_threshold = 0.3\n",
    "\n",
    "filenames = pd.DataFrame({'filename':train_set_generator.filenames.copy()})\n",
    "hard_images = filenames[np.max(probabilities, axis=1) < confidence_threshold]\n",
    "print('Found {} confusing training images at a {} confidence threshold'.format(len(hard_images), confidence_threshold))\n",
    "\n",
    "row = 0\n",
    "for idx in hard_images.sample(frac=1).index:\n",
    "    # Load image\n",
    "    img = Image.open(TRAIN_DIR + hard_images.loc[idx]['filename'])\n",
    "\n",
    "    # Plot image    \n",
    "    axs[row, 0].imshow(img)\n",
    "    #axs[row, 0].set_title('Class: ' + hard_images.loc[idx]['label'])\n",
    "\n",
    "    # Plot probabilities\n",
    "    axs[row, 1].plot(probabilities[idx])\n",
    "    axs[row, 1].set_title('Max:' + str(max(probabilities[idx])))\n",
    "\n",
    "    #print(label)\n",
    "    print(hard_images['filename'][idx])\n",
    "\n",
    "    row += 1\n",
    "    if row > 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#  Confusion matrix\n",
    "#######################################################\n",
    "\n",
    "val_generator.reset()\n",
    "\n",
    "# Get the ground truth\n",
    "filenames = val_generator.filenames.copy()\n",
    "labels = val_generator.labels.copy()\n",
    "\n",
    "# Predict classes\n",
    "probabilities = model.predict(val_generator, verbose=1, workers=2)\n",
    "predictions = np.argmax(probabilities, axis=-1)\n",
    "\n",
    "df_cm = pd.DataFrame({\n",
    "    'img_name': filenames,\n",
    "    'label_truth': labels,\n",
    "    'label_predict': predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "cm = confusion_matrix(df_cm['label_truth'], df_cm['label_predict'])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Show the images of each class to debug bad training\n",
    "# images\n",
    "#######################################################\n",
    "\n",
    "NCOL = 5\n",
    "NROW = 32\n",
    "\n",
    "from PIL import Image\n",
    "fig, axs = plt.subplots(ncols=NCOL, nrows=NROW)\n",
    "fig.set_size_inches(20, 120)\n",
    "\n",
    "label = '12'\n",
    "df = train_labels[train_labels['label'] == label].sample(frac=1)\n",
    "print('Found {} images for class {}'.format(len(df), label))\n",
    "\n",
    "i = 0\n",
    "for row in range(0, NROW):\n",
    "    if i > len(df):\n",
    "        print('No more images')\n",
    "        break\n",
    "\n",
    "    for col in range(0, NCOL):\n",
    "        # Load image\n",
    "        img = Image.open(TRAIN_DIR + df.iloc[i]['img_name'])\n",
    "\n",
    "        # Plot image    \n",
    "        axs[row, col].imshow(img)\n",
    "        axs[row, col].xaxis.set_visible(False)\n",
    "        axs[row, col].yaxis.set_visible(False)\n",
    "        axs[row, col].set_title('Class: ' + df.iloc[i]['img_name'])\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#  Check which classes have the most problems - part 1\n",
    "#######################################################\n",
    "\n",
    "# Predict classes\n",
    "val_generator.reset()\n",
    "probabilities = model.predict(val_generator, verbose=1)\n",
    "predicted_label_indices = np.argmax(probabilities, axis=-1)\n",
    "\n",
    "# Get the ground truth\n",
    "filenames = val_generator.filenames.copy()\n",
    "truth_label_indices = val_generator.labels.copy()\n",
    "\n",
    "label_dict = (train_generator.class_indices)\n",
    "label_dict = dict((v,k) for k,v in label_dict.items())\n",
    "y_pred = [label_dict[k] for k in predicted_label_indices]\n",
    "y_truth = [label_dict[k] for k in truth_label_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#  Check which classes have the most problems - part 2\n",
    "#######################################################\n",
    "\n",
    "df_cm = pd.DataFrame({\n",
    "    'img_name': filenames,\n",
    "    'label_truth': y_truth,\n",
    "    'label_predict': y_pred\n",
    "    })\n",
    "    \n",
    "df_cm['correct'] = False\n",
    "df_cm.loc[df_cm['label_truth'] == df_cm['label_predict'], 'correct'] = True\n",
    "print(len(df_cm[df_cm['correct'] == True]))\n",
    "print(len(df_cm[df_cm['correct'] == False]))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "class_report_mean = df_cm.groupby('label_truth').mean()['correct']\n",
    "class_report_count = df_cm.groupby('label_truth').count()['img_name']\n",
    "#class_report.plot()\n",
    "class_report = pd.DataFrame({\n",
    "    'count': class_report_count,\n",
    "    'correct': class_report_mean\n",
    "    })\n",
    "class_report.sort_values('correct', inplace=True)\n",
    "class_report['correct'].plot()\n",
    "print(class_report[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8cdf8e27a6a01ad1cac13f7a3302b334775911623636020b5b45a311449ddcb2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('python_env_37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
